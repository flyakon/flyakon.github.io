
# üìù Seletected Publications


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/AsiaWheat.png"><img src='image/AsiaWheat.png' alt="AsiaWheat" width="100%"></a></div></div>

<div class='paper-box-text' markdown="1">


<b>Asiawheat: The First Asian 250-M Annual Fractional Wheat Cover Time Series  (2001-2023) Using Convolutional Neural Networks and Transformer Models</b><br>

<i>Available at SSRN</i><br>

<b>Wenyuan Li</b>, Shunlin Liang, Yongzhe Chen, Han Ma, Jianglei Xu, Zhongxin Chen, Husheng Fang, and Fengjiao Zhang<br>


[<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5029097">Paper</a>]<br>

<div style="text-align: justify">

We propose a novel deep learning mapping method that combines Convolutional Neural Networks (CNN) and Transformer models, 
termed DeepMapping, to produce the first annual Asian fractional wheat cover product, AsiaWheat, 
at a 250 m spatial resolution spanning from 2001 to 2023, which includes both winter and spring wheat.

</div>

</div>

</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/DeepPhysiNet.png"><img src='image/DeepPhysiNet.png' alt="DeepPhysiNet" width="100%"></a></div></div>

<div class='paper-box-text' markdown="1">


<b>DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for Accurate and Continuous Weather Modeling</b><br>

<i>arXiv, 2024</i><br>

<b>Wenyuan Li</b>, Zili Liu, Keyan Chen, Hao Chen, Shunlin Liang, Zhengxia Zou, Zhenwei Shi<br>


[<a href="https://arxiv.org/pdf/2401.04125">PDF</a>] [<a href="https://github.com/flyakon/DeepPhysiNet">Github</a>]<br>

<div style="text-align: justify">

We propose a unified framework,
namely <b>DeepPhysiNet</b>, which can incorporate atmospheric
physics into deep learning methods for accurate and continuous weather modeling

</div>

</div>

</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/2022_GeCo.png"><img src='image/2022_GeCo.png' alt="sym" width="100%"></a></div></div>

<div class='paper-box-text' markdown="1">


<b>Geographical Supervision Correction for Remote Sensing Representation Learning</b><br>

<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2022</i><br>

<b>Wenyuan Li</b>, Keyan Chen, and Zhenwei Shi<br>

[<a href="http://levir.buaa.edu.cn/publications/FINAL_VERSION.pdf">PDF</a>]<br>

<div style="text-align: justify">

We propose a Geographical supervision Correction method (GeCo) for remote sensing representation learning. Deviated geographical supervision generated by GLC products can be corrected adaptively using the correction matrix during network pre-training and joint optimization process is designed to simultaneously update the correction matrix and network parameters.

</div>


</div>

</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/2021_GeoKR.png"><img src='image/2021_GeoKR.png' alt="sym" width="100%"></a></div></div>

<div class='paper-box-text' markdown="1">


<b>Geographical Knowledge-Driven Representation Learning for Remote Sensing Images</b><br>

<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2021</i><br>

<b>Wenyuan Li</b>, Keyan Chen, Hao Chen and Zhenwei Shi<br>

[<a href="http://levir.buaa.edu.cn/publications/Geographical_Knowledge-Driven.pdf">PDF</a>] [<a href="https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning">Github</a>]<br>

<div style="text-align: justify">

 We propose a Geographical Knowledge-driven Representation learning method for remote sensing images (GeoKR), improving network performance and reduce the demand for annotated data. The global land cover products and geographical location associated with each remote sensing image are regarded as geographical knowledge to provide supervision for representation learning and network pre-training.

</div>


</div>

</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/SSL.png"><img src='image/SSL.png' alt="sym" width="100%"></a></div></div>

<div class='paper-box-text' markdown="1">


<b>Semantic Segmentation of Remote Sensing Images With Self-Supervised Multitask Representation Learning</b><br>

<i>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (JSTARS), 2021</i><br>

<b>Wenyuan Li</b>, Hao Chen and Zhenwei Shi<br>

[<a href="https://ieeexplore.ieee.org/document/9460820">PDF</a>] [<a href="https://github.com/flyakon/SSLRemoteSensing">Github</a>]<br>

<div style="text-align: justify">

We propose a self-supervised multitask representation learning method to capture effective visual representations of remote sensing images. We design three different pretext tasks and a triplet Siamese network to learn the high-level and low-level image features at the same time. The network can be trained without any labeled data, and the trained model can be fine-tuned with the annotated segmentation dataset

</div>

</div>

</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/gan_cloud.png"><img src='image/gan_cloud.png' alt="sym" width="100%"></a></div></div>

<div class='paper-box-text' markdown="1">


<b>Generative Adversarial Training for Weakly Supervised Cloud Matting</b><br>

<i> International Conference on Computer Vision (ICCV), 2019</i><br>

Zhengxia Zou, <b>Wenyuan Li</b>, Tianyang Shi,  Zhenwei Shi and Jieping Ye.<br>

[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Zou_Generative_Adversarial_Training_for_Weakly_Supervised_Cloud_Matting_ICCV_2019_paper.pdf">PDF</a>] [<a href="https://github.com/flyakon/CloudMattingGAN">Github</a>]<br>

<div style="text-align: justify">

We re-examine the cloud detection under a totally different point of view, i.e. to formulate it as a mixed energy separation process between foreground and background images, which can be equivalently implemented under an image matting paradigm with a clear physical significance. We further propose a generative adversarial framework where the training of our model neither requires any pixel-wise ground truth reference nor any additional user interactions.

</div>

</div>

</div>

